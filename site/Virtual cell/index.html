<!DOCTYPE html>

<html lang="en">


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../MAPK/">
      
      
        <link rel="next" href="../RA/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.12">
    
    
<title>Literature Survey (VPE)</title>

    
      <link rel="stylesheet" href="../assets/stylesheets/main.7e359304.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
  <!-- Add scripts that need to run before here -->
  <!-- Add jquery script -->
  <script src="https://code.jquery.com/jquery-3.7.1.js"></script>
  <!-- Add data table libraries -->
  <script src="https://cdn.datatables.net/2.0.1/js/dataTables.js"></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/2.0.1/css/dataTables.dataTables.css">
  <!-- Load plotly.js into the DOM -->
	<script src='https://cdn.plot.ly/plotly-2.29.1.min.js'></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/buttons/3.0.1/css/buttons.dataTables.css">
  <!-- fixedColumns -->
  <script src="https://cdn.datatables.net/fixedcolumns/5.0.0/js/dataTables.fixedColumns.js"></script>
  <script src="https://cdn.datatables.net/fixedcolumns/5.0.0/js/fixedColumns.dataTables.js"></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/fixedcolumns/5.0.0/css/fixedColumns.dataTables.css">
  <!-- Already specified in mkdocs.yml -->
  <!-- <link rel="stylesheet" href="../docs/custom.css"> -->
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/dataTables.buttons.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.dataTables.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.2.7/pdfmake.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.2.7/vfs_fonts.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.html5.min.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.print.min.js"></script>
  <!-- Google fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
  <!-- Intro.js -->
  <script src="https://cdn.jsdelivr.net/npm/intro.js@7.2.0/intro.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/intro.js@7.2.0/minified/introjs.min.css">


  <!-- 
      
     -->
  <!-- Add scripts that need to run afterwards here -->

    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../custom.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="black">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Literature Survey (VPE)" class="md-header__button md-logo" aria-label="Literature Survey (VPE)" data-md-component="logo">
      
  <img src="../assets/VPE.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Literature Survey (VPE)
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Virtual cell
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="black"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="white" data-md-color-accent="black"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/VirtualPatientEngine/literatureSurveyModels" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    VPE/LiteratureSurveyModels
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../IBD/" class="md-tabs__link">
        
  
    
  
  IBD

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Covid/" class="md-tabs__link">
        
  
    
  
  Covid

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Cancer/" class="md-tabs__link">
        
  
    
  
  Cancer

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../MAPK1/" class="md-tabs__link">
        
  
    
  
  MAPK1

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../MAPK/" class="md-tabs__link">
        
  
    
  
  MAPK

      </a>
    </li>
  

      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
    
  
  Virtual cell

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../RA/" class="md-tabs__link">
        
  
    
  
  RA

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Prostate%20cancer/" class="md-tabs__link">
        
  
    
  
  Prostate cancer

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Virtual%20patients/" class="md-tabs__link">
        
  
    
  
  Virtual patients

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Literature Survey (VPE)" class="md-nav__button md-logo" aria-label="Literature Survey (VPE)" data-md-component="logo">
      
  <img src="../assets/VPE.png" alt="logo">

    </a>
    Literature Survey (VPE)
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/VirtualPatientEngine/literatureSurveyModels" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    VPE/LiteratureSurveyModels
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../IBD/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IBD
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Covid/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Covid
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Cancer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cancer
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../MAPK1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MAPK1
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../MAPK/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MAPK
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Virtual cell
  </span>
  

      </a>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../RA/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RA
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Prostate%20cancer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prostate cancer
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Virtual%20patients/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Virtual patients
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Virtual cell</h1>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2025-01-29 08:54:33 UTC</i>
  </p>

  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <!--
  <div data-intro='Table of contents'>
    <p>
    <h3>Table of Contents</h3>
      <a href="#plot1">1. Citations over time on Virtual cell</a><br>
      <a href="#manually_curated_articles">2. Manually curated articles on Virtual cell</a><br>
      <a href="#recommended_articles">3. Recommended articles on Virtual cell</a><br>
    <p>
  </div>

  <div data-intro='Plot displaying number of citations over time 
                  on the given topic based on recommended articles'>
    <p>
    <h3 id="plot1">1. Citations over time on Virtual cell</h3>
      <div id='myDiv1'>
      </div>
    </p>
  </div>
  -->

  <div data-intro='Manually curated articles on the given topic'>
    <p>
    <h3 id="manually_curated_articles">Manually curated articles on <i>Virtual cell</i></h3>
    <table id="table1" class="display" style="width:100%">
    <thead>
      <tr>
          <th data-intro='Click to view the abstract (if available)'>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/ Conference</th>
          <th>Citation count</th>
          <th data-intro='Highest h-index among the authors'>Highest h-index</th>
          <th data-intro='Recommended articles extracted by considering
                          only the given article'>
              View recommendations
              </th>
      </tr>
    </thead>
    <tbody>

        <tr id="The cell is arguably the most fundamental unit of life and is central to understanding biology. Accurate modeling of cells is important for this understanding as well as for determining the root causes of disease. Recent advances in artificial intelligence (AI), combined with the ability to generate large-scale experimental data, present novel opportunities to model cells. Here we propose a vision of leveraging advances in AI to construct virtual cells, high-fidelity simulations of cells and cellular systems under different conditions that are directly learned from biological data across measurements and scales. We discuss desired capabilities of such AI Virtual Cells, including generating universal representations of biological entities across scales, and facilitating interpretable in silico experiments to predict and understand their behavior using Virtual Instruments. We further address the challenges, opportunities and requirements to realize this vision including data needs, evaluation strategies, and community standards and engagement to ensure biological accuracy and broad utility. We envision a future where AI Virtual Cells help identify new drug targets, predict cellular responses to perturbations, as well as scale hypothesis exploration. With open science collaborations across the biomedical ecosystem that includes academia, philanthropy, and the biopharma and AI industries, a comprehensive predictive understanding of cell mechanisms and interactions has come into reach.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/18c7d4a106a6889e81e970fd01cdcd8fbf13415c" target='_blank'>
                How to Build the Virtual Cell with Artificial Intelligence: Priorities and Opportunities
                </a>
              </td>
          <td>
            Charlotte Bunne, Yusuf Roohani, Yanay Rosen, Ankit Gupta, Xikun Zhang, Marcel Roed, Theo Alexandrov, Mohammed AlQuraishi, Patricia Brennan, Daniel B. Burkhardt, Andrea Califano, J. Cool, A. Dernburg, Kirsty Ewing, Emily B. Fox, Matthias Haury, Amy E. Herr, Eric Horvitz, Patrick D. Hsu, Viren Jain, Gregory R. Johnson, Thomas Kalil, David R. Kelley, S. Kelley, A. Kreshuk, Tim Mitchison, Stephani Otte, Jay Shendure, Nicholas J Sofroniew, Fabian Theis, Christina V. Theodoris, S. Upadhyayula, M. Valer, Bo Wang, Eric Xing, S. Yeung-Levy, M. Zitnik, Theofanis Karaletsos, Aviv Regev, Emma Lundberg, J. Leskovec, Stephen R. Quake
          </td>
          <td>2024-09-18</td>
          <td>ArXiv, arXiv.org</td>
          <td>4</td>
          <td>136</td>

            <td><a href='../recommendations/18c7d4a106a6889e81e970fd01cdcd8fbf13415c' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

    </tbody>
    <tfoot>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/ Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
          <th>View recommendations</th>
      </tr>
    </tfoot>
    </table>
    </p>
  </div>

  <div data-intro='Recommended articles extracted by contrasting
                  articles that are relevant against not relevant for Virtual cell'>
    <p>
    <h3 id="recommended_articles">Recommended articles on <i>Virtual cell</i></h3>
    <table id="table2" class="display" style="width:100%">
    <thead>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
      </tr>
    </thead>
    <tbody>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/314bace3b5ecee749f382a2813d0312d23d0a989" target='_blank'>
              Build the virtual cell with artificial intelligence: a perspective for cancer research
              </a>
            </td>
          <td>
            Tao Yang, , Fei Ma, , Haili Qian
          </td>
          <td>2025-01-27</td>
          <td>Military Medical Research</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="ABSTRACT Dynamic modeling of cellular states has emerged as a pivotal approach for understanding complex biological processes such as cell differentiation, disease progression, and tissue development. This review provides a comprehensive overview of current approaches for modeling cellular state dynamics, focusing on techniques ranging from dynamic or static biomolecular network models to deep learning models. We highlight how these approaches integrated with various omics data such as transcriptomics, and single-cell RNA sequencing could be used to capture and predict cellular behavior and transitions. We also discuss applications of these modeling approaches in predicting gene knockout effects, designing targeted interventions, and simulating organ development. This review emphasizes the importance of selecting appropriate modeling strategies based on scalability and resolution requirements, which vary according to the complexity and size of biological systems under study. By evaluating strengths, limitations, and recent advancements of these methodologies, we aim to guide future research in developing more robust and interpretable models for understanding and manipulating cellular state dynamics in various biological contexts, ultimately advancing therapeutic strategies and precision medicine.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c33d2d9327e5accdbe556a83bf9aad32ab2eced5" target='_blank'>
              Advances in modeling cellular state dynamics: integrating omics data and predictive techniques
              </a>
            </td>
          <td>
            Sungwon Jung
          </td>
          <td>2025-01-10</td>
          <td>Animal Cells and Systems</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The convergence of Artificial Intelligence (AI) and neuroscience is redefining our understanding of the brain, unlocking new possibilities in research, diagnosis, and therapy. This review explores how AI’s cutting-edge algorithms—ranging from deep learning to neuromorphic computing—are revolutionizing neuroscience by enabling the analysis of complex neural datasets, from neuroimaging and electrophysiology to genomic profiling. These advancements are transforming the early detection of neurological disorders, enhancing brain–computer interfaces, and driving personalized medicine, paving the way for more precise and adaptive treatments. Beyond applications, neuroscience itself has inspired AI innovations, with neural architectures and brain-like processes shaping advances in learning algorithms and explainable models. This bidirectional exchange has fueled breakthroughs such as dynamic connectivity mapping, real-time neural decoding, and closed-loop brain–computer systems that adaptively respond to neural states. However, challenges persist, including issues of data integration, ethical considerations, and the “black-box” nature of many AI systems, underscoring the need for transparent, equitable, and interdisciplinary approaches. By synthesizing the latest breakthroughs and identifying future opportunities, this review charts a path forward for the integration of AI and neuroscience. From harnessing multimodal data to enabling cognitive augmentation, the fusion of these fields is not just transforming brain science, it is reimagining human potential. This partnership promises a future where the mysteries of the brain are unlocked, offering unprecedented advancements in healthcare, technology, and beyond.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1b0a679460d43d67d16597bd4314fe5415d679fe" target='_blank'>
              Artificial Intelligence and Neuroscience: Transformative Synergies in Brain Research and Clinical Applications
              </a>
            </td>
          <td>
            Răzvan Onciul, Catalina-Ioana Tataru, A. Dumitru, Carla Crivoi, Matei Serban, Razvan-Adrian Covache-Busuioc, M. Radoi, C. Toader
          </td>
          <td>2025-01-01</td>
          <td>Journal of Clinical Medicine</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Spatial proteomics technologies have transformed our understanding of complex tissue architectures by enabling simultaneous analysis of multiple molecular markers and their spatial organization. The high dimensionality of these data, varying marker combinations across experiments and heterogeneous study designs pose unique challenges for computational analysis. Here, we present Virtual Tissues (VirTues), a foundation model framework for biological tissues that operates across the molecular, cellular and tissue scale. VirTues introduces innovations in transformer architecture design, including a novel tokenization scheme that captures both spatial and marker dimensions, and attention mechanisms that scale to high-dimensional multiplex data while maintaining interpretability. Trained on diverse cancer and non-cancer tissue datasets, VirTues demonstrates strong generalization capabilities without task-specific fine-tuning, enabling cross-study analysis and novel marker integration. As a generalist model, VirTues outperforms existing approaches across clinical diagnostics, biological discovery and patient case retrieval tasks, while providing insights into tissue function and disease mechanisms.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2a2d0e28b69142ec2e831c69f38b94d5b7ee18b8" target='_blank'>
              AI-powered virtual tissues from spatial proteomics for clinical diagnostics and biomedical discovery
              </a>
            </td>
          <td>
            Johann Wenckstern, Eeshaan Jain, Kiril Vasilev, Matteo Pariset, Andreas Wicki, Gabriele Gut, Charlotte Bunne
          </td>
          <td>2025-01-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="The latest breakthroughs in information technology and biotechnology have catalyzed a revolutionary shift within the modern healthcare landscape, with notable impacts from artificial intelligence (AI) and deep learning (DL). Particularly noteworthy is the adept application of large language models (LLMs), which enable seamless and efficient communication between scientific researchers and AI systems. These models capitalize on neural network (NN) architectures that demonstrate proficiency in natural language processing, thereby enhancing interactions. This comprehensive review outlines the cutting-edge advancements in the application of LLMs within the pharmaceutical industry, particularly in drug development. It offers a detailed exploration of the core mechanisms that drive these models and zeroes in on the practical applications of several models that show great promise in this domain. Additionally, this review delves into the pivotal technical and ethical challenges that arise with the practical implementation of LLMs. There is an expectation that LLMs will assume a more pivotal role in the development of innovative drugs and will ultimately contribute to the accelerated development of revolutionary pharmaceuticals.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d7157f73a59c3a619549217e6d27884bebedf419" target='_blank'>
              Large language models facilitating modern molecular biology and novel drug development
              </a>
            </td>
          <td>
            Xiao-huan Liu, Zhen-hua Lu, Tao Wang, Fei Liu
          </td>
          <td>2024-12-24</td>
          <td>Frontiers in Pharmacology</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Many cellular processes involve information processing and decision making. We can probe these processes at increasing molecular detail. The analysis of heterogeneous data remains a challenge that requires new ways of thinking about cells in quantitative, predictive, and mechanistic ways. We discuss the role of mathematical models in the context of cell-fate decision making systems across the tree of life. Complex multi-cellular organisms have been a particular focus, but single celled organisms also have to sense and respond to their environment. We center our discussion around the idea of design principles which we can learn from observations and modeling, and exploit in order to (re)-design or guide cellular behavior.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3c4b2daa20b8286a47949426e0feca8647a87870" target='_blank'>
              Mapping, modeling, and reprogramming cell-fate decision making systems
              </a>
            </td>
          <td>
            Lucy Ham, Taylor E. Woodford, Megan A. Coomer, M. P. Stumpf
          </td>
          <td>2024-12-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="The exponential increase in biomedical data offers unprecedented opportunities for drug discovery, yet overwhelms traditional data analysis methods, limiting the pace of new drug development. Here we introduce a framework for autonomous artificial intelligence (AI)-driven drug discovery that integrates knowledge graphs with large language models (LLMs). It is capable of planning and carrying out automated drug discovery programs at a massive scale while providing details of its research strategy, progress, and all supporting data. At the heart of this framework lies the focal graph - a novel construct that harnesses centrality algorithms to distill vast, noisy datasets into concise, transparent, data-driven hypotheses. We demonstrate that even small-scale applications of this highly scalable approach can yield novel, transparent insights relevant to multiple stages of the drug discovery process and present a prototype system which autonomously plans and executes a multi-step target discovery workflow. Graphical Abstract">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/25e02fdf899bb13ba0ee43cd45418d55fee11bb3" target='_blank'>
              A Framework for Autonomous AI-Driven Drug Discovery
              </a>
            </td>
          <td>
            Douglas W. Selinger, Timothy R. Wall, Eleni Stylianou, Ehab M. Khalil, Jedidiah Gaetz, Oren Levy
          </td>
          <td>2024-12-21</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Understanding perturbations at the single-cell level is essential for unraveling cellular mechanisms and their implications in health and disease. The growing availability of biological data has driven the development of a variety of in silico perturbation methods designed for single-cell analysis, which offer a means to address many inherent limitations of experimental approaches. However, these computational methods are often tailored to specific scenarios and validated on limited datasets and metrics, making their evaluation and comparison challenging. In this work, we introduce a comprehensive benchmarking framework to systematically evaluate in silico perturbation methods across four key scenarios: predicting effects of unseen perturbations in known cell types, predicting effects of observed perturbations in unseen cell types, zero-shot transfer to bulk RNA-seq of cell lines, and application to real-world biological cases. For each scenario, we curated diverse and abundant datasets, standardizing them into flexible formats to enable efficient analysis. Additionally, we developed multiple metrics tailored to each scenario, facilitating a thorough and comparative evaluation of these methods. Our benchmarking study assessed 10 methods, ranging from linear baselines to advanced machine learning approaches, across these scenarios. While some methods demonstrated surprising efficacy in specific contexts, significant challenges remain, particularly in zero-shot predictions and the modeling of complex biological processes. This work provides a valuable resource for evaluating and improving in silico perturbation methods, serving as a foundation for bridging computational predictions with experimental validation and real-world biological applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3bbce36757454626364ec11e5ffa0b515e41cf92" target='_blank'>
              Benchmarking AI Models for In Silico Gene Perturbation of Cells
              </a>
            </td>
          <td>
            Chen Li, Haoxiang Gao, Yuli She, Haiyang Bian, Qing Chen, Kai Liu, Lei Wei, Xuegong Zhang
          </td>
          <td>2025-01-07</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Large-scale foundation models have recently opened new avenues for artificial general intelligence. Such a research paradigm has recently shown considerable promise in the analysis of single-cell sequencing data, while to date, efforts have centered on transcriptome. In contrast to gene expression, chromatin accessibility provides more decisive insights into cell states, shaping the chromatin regulatory landscapes that control transcription in distinct cell types. Yet, challenges also persist due to the abundance of features, high data sparsity, and the quasi-binary nature of these data. Here, we introduce EpiAgent, the first foundation model for single-cell epigenomic data, pretrained on a large-scale Human-scATAC-Corpus comprising approximately 5 million cells and 35 billion tokens. EpiAgent encodes chromatin accessibility patterns of cells as concise “cell sentences,” and employs bidirectional attention to capture cellular heterogeneity behind regulatory networks. With comprehensive benchmarks, we demonstrate that EpiAgent excels in typical downstream tasks, including unsupervised feature extraction, supervised cell annotation, and data imputation. By incorporating external embeddings, EpiAgent facilitates the prediction of cellular responses to both out-of-sample stimulated and unseen genetic perturbations, as well as reference data integration and query data mapping. By simulating the knockout of key cis-regulatory elements, EpiAgent enables in-silico treatment for cancer analysis. We further extended zero-shot capabilities of EpiAgent, allowing direct cell type annotation on newly sequenced datasets without additional training.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d6ea09eb2bc05a10c8fbc488c6d30f15102003fd" target='_blank'>
              EpiAgent: Foundation model for single-cell epigenomic data
              </a>
            </td>
          <td>
            Xiaoyang Chen, Keyi Li, Xuejian Cui, Zian Wang, Qun Jiang, Jiacheng Lin, Zhen Li, Zijing Gao, Rui Jiang
          </td>
          <td>2024-12-21</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Cells are the functional units of life, and the wide range of biological functions they perform are orchestrated by myriad molecular interactions within an intricate subcellular architecture. This cellular organization and functionality can be studied with microscopy at scale, and machine learning has become a powerful tool for interpreting the rich information in these images. Here, we introduce SubCell, a suite of self-supervised deep learning models for fluorescence microscopy that are designed to accurately capture cellular morphology, protein localization, cellular organization, and biological function beyond what humans can readily perceive. These models were trained using the metadata-rich, proteome-wide image collection from the Human Protein Atlas. SubCell outperforms state-of-the-art methods across a variety of tasks relevant to single-cell biology. Remarkably, SubCell generalizes to other fluorescence microscopy datasets without any finetuning, including dataset of drug-perturbed cells, where SubCell accurately predicts drug perturbations of cancer cells and mechanisms of action. Finally, we construct the first proteome-wide hierarchical map of proteome organization that is directly learned from image data. This vision-based multiscale cell map defines cellular subsystems with large protein-complex resolution, reveals proteins with similar functions, and distinguishes dynamic and stable behaviors within cellular compartments. In conclusion, SubCell enables deep image-driven representations of cellular architecture applicable across diverse biological contexts and datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6851517fa623f4a3acb5eba2910634449d37336f" target='_blank'>
              SubCell: Vision foundation models for microscopy capture single-cell biology
              </a>
            </td>
          <td>
            Ankit Gupta, Zoe Wefers, Konstantin Kahnert, J. N. Hansen, William D. Leineweber, Anthony J. Cesnik, Dan Lu, Ulrika Axelsson, Frederic Ballllosera Navarro, Theofanis Karaletsos, Emma Lundberg
          </td>
          <td>2024-12-08</td>
          <td>bioRxiv</td>
          <td>1</td>
          <td>17</td>
        </tr>

        <tr id="This review focuses on opportunities and challenges of future AI developments in veterinary medicine, from the perspective of computer science researchers in developing AI systems for animal behavior analysis. We examine the paradigms of supervised learning, self-supervised learning, and foundation models, highlighting their applications and limitations in automating animal behavior analysis. These emerging technologies present future challenges in data, modeling, and evaluation in veterinary medicine. To address this, we advocate for a collaborative approach that integrates the expertise of AI researchers, veterinary professionals, and other stakeholders to navigate the evolving landscape of AI in veterinary medicine. Through cross-domain dialogue and an emphasis on human and animal well-being, we can shape AI development to advance veterinary practice for the benefit of all.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2732f9c17090b4dfb097dcfcd0e79b1eb6b6a362" target='_blank'>
              Toward collaborative artificial intelligence development for animal well-being.
              </a>
            </td>
          <td>
            Jennifer J Sun
          </td>
          <td>2025-01-10</td>
          <td>Journal of the American Veterinary Medical Association</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Developing a unified model of cellular systems is a canonical challenge in biology. Recently, a wealth of public single-cell RNA sequencing data as well as rapid scaling of self-supervised learning methods have provided new avenues to address this longstanding challenge. However, rapid parameter scaling has been essential to the success of large language models in text and images, while similar scaling has not been attempted with Transformer architectures for cellular modeling. To produce accurate, transferable, and biologically meaningful representations of cellular systems, we develop AIDO.Cell, a pretrained module for representing gene expression and cellular systems in an AI-driven Digital Organism [1]. AIDO.Cell contains a series of 3M, 10M, 100M, and 650M parameter encoder-only dense Transformer models pre-trained on 50 million human cells from diverse tissues using a read-depth-aware masked gene expression pretraining objective. Unlike previous models, AIDO.Cell is capable of handling the entire human transcriptome as input without truncation or sampling tricks, thus learning accurate and general representations of the human cell’s entire transcriptional context. This pretraining with a longer context was enabled through FlashAttention-2, mixed precision, and large-scale distributed systems training. AIDO.Cell (100M) achieves state-of-the-art results in tasks such as zero-shot clustering, cell-type classification, and perturbation modeling. Our findings reveal interesting loss scaling behaviors as we increase AIDO.Cell’s parameters from 3M to 650M, providing insights for future directions in single-cell modeling. Models and code are available through ModelGenerator in https://github.com/genbio-ai/AIDO and on Hugging Face.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/030fe4dfa022210969c571ec0105ec9c64fea118" target='_blank'>
              Scaling Dense Representations for Single Cell with Transcriptome-Scale Context
              </a>
            </td>
          <td>
            Nicholas Ho, Caleb N. Ellington, Jinyu Hou, Sohan Addagudi, Shentong Mo, Tianhua Tao, Dian Li, Yonghao Zhuang, Hongyi Wang, Xingyi Cheng, Le Song, Eric P. Xing
          </td>
          <td>2024-12-03</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Image-based profiling is rapidly transforming drug discovery, offering unprecedented insights into cellular responses. However, experimental variability hinders accurate identification of mechanisms of action (MoA) and compound targets. Existing methods commonly fail to generalize to novel compounds, limiting their utility in exploring uncharted chemical space. To address this, we present a confounder-aware foundation model integrating a causal mechanism within a latent diffusion model, enabling the generation of balanced synthetic datasets for robust biological effect estimation. Trained on over 13 million Cell Painting images and 107 thousand compounds, our model learns robust cellular phenotype representations, mitigating confounder impact. We achieve state-of-the-art MoA and target prediction for both seen (0.66 and 0.65 ROC-AUC) and unseen compounds (0.65 and 0.73 ROC-AUC), significantly surpassing real and batch-corrected data. This innovative framework advances drug discovery by delivering robust biological effect estimations for novel compounds, potentially accelerating hit expansion. Our model establishes a scalable and adaptable foundation for cell imaging, holding the potential to become a cornerstone in data-driven drug discovery.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/70e5181b7f0c0ceeecbeb4bfde45a1c946b950b6" target='_blank'>
              Confounder-aware foundation modeling for accurate phenotype profiling in cell imaging
              </a>
            </td>
          <td>
            G. Papanastasiou, Pedro P. Sanchez, Argyrios Christodoulidis, Guang Yang, W. H. L. Pinaya
          </td>
          <td>2024-12-23</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="Understanding cellular responses to genetic perturbations is essential for understanding gene regulation and phenotype formation. While high-throughput single-cell RNA-sequencing has facilitated detailed profiling of heterogeneous transcriptional responses to perturbations at the single-cell level, there remains a pressing need for computational models that can decode the mechanisms driving these responses and accurately predict outcomes to prioritize target genes for experimental design. Here, we present scLAMBDA, a deep generative learning framework designed to model and predict single-cell transcriptional responses to genetic perturbations, including single-gene and combinatorial multi-gene perturbations. By leveraging gene embeddings derived from large language models, scLAMBDA effectively integrates prior biological knowledge and disentangles basal cell states from perturbation-specific salient representations. Through comprehensive evaluations on multiple single-cell CRISPR Perturb-seq datasets, scLAMBDA consistently outperformed state-of-the-art methods in predicting perturbation outcomes, achieving higher prediction accuracy. Notably, scLAMBDA demonstrated robust generalization to unseen target genes and perturbations, and its predictions captured both average expression changes and the heterogeneity of single-cell responses. Furthermore, its predictions enable diverse downstream analyses, including the identification of differentially expressed genes and the exploration of genetic interactions, demonstrating its utility and versatility.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/22a55ef8d91811a0ec7e070fee512d93b735206f" target='_blank'>
              Modeling and predicting single-cell multi-gene perturbation responses with scLAMBDA
              </a>
            </td>
          <td>
            Gefei Wang, Tianyu Liu, Jia Zhao, Youshu Cheng, Hongyu Zhao
          </td>
          <td>2024-12-08</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Considering the significance of proteins, computational protein science has always been a critical scientific field, dedicated to revealing knowledge and developing applications within the protein sequence-structure-function paradigm. In the last few decades, Artificial Intelligence (AI) has made significant impacts in computational protein science, leading to notable successes in specific protein modeling tasks. However, those previous AI models still meet limitations, such as the difficulty in comprehending the semantics of protein sequences, and the inability to generalize across a wide range of protein modeling tasks. Recently, LLMs have emerged as a milestone in AI due to their unprecedented language processing&generalization capability. They can promote comprehensive progress in fields rather than solving individual tasks. As a result, researchers have actively introduced LLM techniques in computational protein science, developing protein Language Models (pLMs) that skillfully grasp the foundational knowledge of proteins and can be effectively generalized to solve a diversity of sequence-structure-function reasoning problems. While witnessing prosperous developments, it's necessary to present a systematic overview of computational protein science empowered by LLM techniques. First, we summarize existing pLMs into categories based on their mastered protein knowledge, i.e., underlying sequence patterns, explicit structural and functional information, and external scientific languages. Second, we introduce the utilization and adaptation of pLMs, highlighting their remarkable achievements in promoting protein structure prediction, protein function prediction, and protein design studies. Then, we describe the practical application of pLMs in antibody design, enzyme design, and drug discovery. Finally, we specifically discuss the promising future directions in this fast-growing field.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0322d6eef1567b8b6ce3998d03f955082f9a87b6" target='_blank'>
              Computational Protein Science in the Era of Large Language Models (LLMs)
              </a>
            </td>
          <td>
            Wenqi Fan, Yi Zhou, Shijie Wang, Yuyao Yan, Hui Liu, Qian Zhao, Le Song, Qing Li
          </td>
          <td>2025-01-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Predicting single-cell transcriptomes following perturbation is crucial for understanding gene regulation and guiding drug discovery. Yet, the complexity of perturbation effects pose significant challenges for robust predictive modeling. Although recent efforts have introduced foundation models alongside traditional statistical and machine learning approaches, a comprehensive benchmarking study of their predictive performance has been lacking. Here we establish a standardized evaluation framework and systematically assess 9 single-cell perturbation prediction models on 17 diverse datasets, spanning multiple cell types and perturbation modalities. Leveraging a multifaceted suite of 24 evaluation metrics, we find that models often excel in capturing global expression profiles yet struggle to predict the nuanced effects of perturbed genes, or vice versa. Moreover, while foundation models frequently outperform simpler methods, they tend to converge on population averages, struggling to capture heterogeneous cellular responses to perturbations. Taken together, our study highlights both the potential and limitations of single-cell foundation models, identifies opportunities for future improvement, and provide a roadmap for advancing single-cell predictive biology.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b5e97201921057d49321deff06c2abc6d0f582ba" target='_blank'>
              A Systematic Comparison of Single-Cell Perturbation Response Prediction Models
              </a>
            </td>
          <td>
            Lanxiang Li, Yue You, Wenyu Liao, Xueying Fan, Shihong Lu, Ye Cao, Bo Li, Wenle Ren, Yunlin Fu, Jiaming Kong, Shuangjia Zheng, Jizheng Chen, Xiaodong Liu, Luyi Tian
          </td>
          <td>2024-12-23</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Identifying the genetic and molecular drivers of phenotypic heterogeneity among individuals is vital for understanding human health and for diagnosing, monitoring, and treating diseases. To this end, international consortia such as the Human Cell Atlas and the Tabula Sapiens are creating comprehensive cellular references. Due to the massive volume of data generated, machine learning methods, especially transformer architectures, have been widely employed in related studies. However, applying machine learning to cellular data presents several challenges. One such challenge is making the methods interpretable with respect to both the input cellular information and its context. Another less explored challenge is the accurate representation of cells outside existing references, referred to as out-of-distribution (OOD) cells. The out-of-distribution could be attributed to various physiological conditions, such as comparing diseased cells, particularly tumor cells, with healthy reference data, or significant technical variations, such as using transfer learning from single-cell reference to spatial query data. Inspired by the global workspace theory in cognitive neuroscience, we introduce CellMemory, a bottlenecked Transformer with improved generalization capabilities designed for the hierarchical interpretation of OOD cells unseen during reference building. Even without pre-training, it exceeds the performance of large language models pre-trained with tens of millions of cells. In particular, when deciphering spatially resolved single-cell transcriptomics data, CellMemory demonstrates the ability to interpret data at the granule level accurately. Finally, we harness CellMemory’s robust representational capabilities to elucidate malignant cells and their founder cells in different patients, providing reliable characterizations of the cellular changes caused by the disease.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9a2f0dfed3cd6527713cbb3e66fe456d8f0b2727" target='_blank'>
              Hierarchical Interpretation of Out-of-Distribution Cells Using Bottlenecked Transformer
              </a>
            </td>
          <td>
            Qifei Wang, He Zhu, Yiwen Hu, Yanjie Chen, Yuwei Wang, Xuegong Zhang, James Zou, M. Kellis, Yue Li, Dianbo Liu, Lan Jiang
          </td>
          <td>2024-12-20</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>37</td>
        </tr>

        <tr id="Over the past decade, the revolution in single-cell sequencing has enabled the simultaneous molecular profiling of various modalities across thousands of individual cells, allowing scientists to investigate the diverse functions of complex tissues and uncover underlying disease mechanisms. Among all the analytical steps, assigning individual cells to specific types is fundamental for understanding cellular heterogeneity. However, this process is usually labor-intensive and requires extensive expert knowledge. Recent advances in large language models (LLMs) have demonstrated their ability to efficiently process and synthesize vast corpora of text to automatically extract essential biological knowledge, such as marker genes, potentially promoting more efficient and automated cell type annotations. To thoroughly evaluate the capability of modern instruction-tuned LLMs in automating the cell type identification process, we introduce SOAR, a comprehensive benchmarking study of LLMs for cell type annotation tasks in single-cell genomics. Specifically, we assess the performance of 8 instruction-tuned LLMs across 11 datasets, spanning multiple cell types and species. Our study explores the potential of LLMs to accurately classify and annotate cell types in single-cell RNA sequencing (scRNA-seq) data, while extending their application to multiomics data through cross-modality translation. Additionally, we evaluate the effectiveness of chain-of-thought (CoT) prompting techniques in generating detailed biological insights during the annotation process. The results demonstrate that LLMs can provide robust interpretations of single-cell data without requiring additional fine-tuning, advancing the automation of cell type annotation in genomics research.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f47da8deb537b469f4090808055a63f133951a25" target='_blank'>
              Single-Cell Omics Arena: A Benchmark Study for Large Language Models on Cell Type Annotation Using Single-Cell Data
              </a>
            </td>
          <td>
            Junhao Liu, Siwei Xu, Lei Zhang, Jing Zhang
          </td>
          <td>2024-12-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="The development of single-cell and spatial transcriptomics has revolutionized our capacity to investigate cellular properties, functions, and interactions in both cellular and spatial contexts. However, the analysis of single-cell and spatial omics data remains challenging. First, single-cell sequencing data are high-dimensional and sparse, often contaminated by noise and uncertainty, obscuring the underlying biological signals. Second, these data often encompass multiple modalities, including gene expression, epigenetic modifications, and spatial locations. Integrating these diverse data modalities is crucial for enhancing prediction accuracy and biological interpretability. Third, while the scale of single-cell sequencing has expanded to millions of cells, high-quality annotated datasets are still limited. Fourth, the complex correlations of biological tissues make it difficult to accurately reconstruct cellular states and spatial contexts. Traditional feature engineering-based analysis methods struggle to deal with the various challenges presented by intricate biological networks. Deep learning has emerged as a powerful tool capable of handling high-dimensional complex data and automatically identifying meaningful patterns, offering significant promise in addressing these challenges. This review systematically analyzes these challenges and discusses related deep learning approaches. Moreover, we have curated 21 datasets from 9 benchmarks, encompassing 58 computational methods, and evaluated their performance on the respective modeling tasks. Finally, we highlight three areas for future development from a technical, dataset, and application perspective. This work will serve as a valuable resource for understanding how deep learning can be effectively utilized in single-cell and spatial transcriptomics analyses, while inspiring novel approaches to address emerging challenges.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3cded8f2e52e0d5c474348d064460c5920c4018a" target='_blank'>
              Deep Learning in Single-Cell and Spatial Transcriptomics Data Analysis: Advances and Challenges from a Data Science Perspective
              </a>
            </td>
          <td>
            Shuang Ge, Shuqing Sun, Huan Xu, Qiang Cheng, Zhixiang Ren
          </td>
          <td>2024-12-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Inferring cell-type-specific gene regulatory networks (GRNs) from single-cell RNA sequencing (scRNA-seq) data is a complex task, primarily due to data sparsity, noise, and the dynamic, context-dependent nature of gene regulation across cell types and states. Recent advancements in the collection of experimentally validated data on transcription factor binding have facilitated GRN inference via supervised machine learning methods—where models learn from known TF-gene pairs to guide predictions. However, these methods still face challenges in 1) effectively representing and integrating prior knowledge, and 2) capturing regulatory mechanisms across diverse cellular contexts. To tackle the above challenges, we introduce a novel GRN inference method, scRegNet, that learns a joint representation from graph neural networks (GNNs) and pre-trained single-cell foundation models (scFMs). scRegNet combines rich contextual representations learned by large-scale, single-cell foundation models—trained on extensive unlabeled scRNA-seq datasets—with the structured knowledge embedded in experimentally validated networks through GNNs. This integration enables robust inference—the prediction of unknown gene regulatory interactions—by simultaneously accounting for gene expression patterns and established gene regulatory networks. We evaluated our approach on seven single-cell scRNA-seq benchmark datasets from the BEELINE study [22], outperforming current state-of-the-art methods in cell-type-specific GRN inference. scRegNet demonstrates a superior ability to capture intricate regulatory interactions between genes across various cell types, providing a more in-depth understanding of cellular processes and regulatory dynamics. By harnessing the capabilities of large-scale pre-trained single-cell foundation models and GNNs, scRegNet offers a scalable and adaptable tool for advancing research in cell type-specific gene interactions and biological functions. Code Availability https://github.com/sindhura-cs/scRegNet">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a1f6744d38931653b12a394db70edd9fa7445d36" target='_blank'>
              Gene Regulatory Network Inference with Joint Representation from Graph Neural Network and Single-Cell Foundation Model
              </a>
            </td>
          <td>
            Sindhura Kommu, Yi‐Zheng Wang, Yue Wang, Xuan Wang
          </td>
          <td>2024-12-20</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Abstract Motivation Recent experimental developments enable single-cell multimodal epigenomic profiling, which measures multiple histone modifications and chromatin accessibility within the same cell. Such parallel measurements provide exciting new opportunities to investigate how epigenomic modalities vary together across cell types and states. A pivotal step in using these types of data is integrating the epigenomic modalities to learn a unified representation of each cell, but existing approaches are not designed to model the unique nature of this data type. Our key insight is to model single-cell multimodal epigenome data as a multichannel sequential signal. Results We developed ConvNet-VAEs, a novel framework that uses one-dimensional (1D) convolutional variational autoencoders (VAEs) for single-cell multimodal epigenomic data integration. We evaluated ConvNet-VAEs on nano-CUT&Tag and single-cell nanobody-tethered transposition followed by sequencing data generated from juvenile mouse brain and human bone marrow. We found that ConvNet-VAEs can perform dimension reduction and batch correction better than previous architectures while using significantly fewer parameters. Furthermore, the performance gap between convolutional and fully connected architectures increases with the number of modalities, and deeper convolutional architectures can increase the performance, while the performance degrades for deeper fully connected architectures. Our results indicate that convolutional autoencoders are a promising method for integrating current and future single-cell multimodal epigenomic datasets. Availability and implementation The source code of VAE models and a demo in Jupyter notebook are available at https://github.com/welch-lab/ConvNetVAE">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/20ecdaf3f88c449a2add94564a76d67968878081" target='_blank'>
              Integrating single-cell multimodal epigenomic data using 1D convolutional neural networks
              </a>
            </td>
          <td>
            Chao Gao, Joshua D. Welch
          </td>
          <td>2025-01-01</td>
          <td>Bioinformatics</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Single-cell mosaic integration has revolutionized our understanding of cellular heterogeneity, offering unprecedented resolution into cellular states and contexts. While large language models (LLMs) have achieved success in the analysis of single-cell omics data, their application specifically focused on the integration of mosaic data remains limited. Current computational approaches often fall short in addressing these challenges. They frequently depend on assumptions of data completeness and uniform quality, failing to manage the variability and noise effectively introduced by missing data. Moreover, these models might not efficiently process large, diverse datasets or account for biological heterogeneity and long-ranging dependencies across various data types. To address these shortcomings, we introduce the Single-cell Mosaic Omics Nonlinear Integration and Clustering Analysis (scMonica) framework, which employs a LSTM-transformer hybrid architecture. This innovative model combines the strengths of Long Short-Term Memory (LSTM) networks, which excel at capturing long-range dependencies within sequential gene expression patterns, with transformers, renowned for their attention mechanisms that handle the complex, non-linear interactions characteristic of multi-layered datasets. By leveraging these complementary strengths, our approach enhances the integration process significantly, allowing for nuanced management of the intrinsic heterogeneity and sparsity of mosaic datasets. Comprehensive evaluations demonstrate the robustness and effectiveness of our approach, offering unparalleled versatility and accuracy in multi-omics data analysis. These advancements underscore scMonica’s potential to drive significant insights in single-cell developmental biology, oncology, and beyond. We discuss the underlying technologies, analyze their applications, and contemplate future directions that promise to extend the boundaries of both research and clinical domains.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/931cde7f3a8ef5a345bbae1fc1a0d1f9d38834b1" target='_blank'>
              scMonica: Single-cell Mosaic Omics Nonlinear Integration and Clustering Analysis
              </a>
            </td>
          <td>
            Xiaoli Li, Rui Zhang, Saba Aslam, Huijun Li, Yuxi Chen, Zequn Zhang, Ruey-Song Huang, Hongyan Wu
          </td>
          <td>2024-12-03</td>
          <td>2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/312a168bd9c35c74bf48490a99836b14ea6ab8d8" target='_blank'>
              Challenges of reproducible AI in biomedical data science
              </a>
            </td>
          <td>
            Henry Han
          </td>
          <td>2025-01-10</td>
          <td>BMC Medical Genomics</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Single-cell genomics enables the study of cell states and cell state transitions across biological conditions like aging, drug treatment, or injury. However, existing computational methods often struggle to simultaneously disentangle shared and condition-specific transcriptional patterns, particularly in experimental designs with missing data, unmatched cell populations, or complex attribute combinations. To address these challenges, Patches identifies universal transcriptomic features alongside condition-dependent variations in scRNA-seq data. Using conditional subspace learning, Patches enables robust integration, cross-condition prediction, and biologically interpretable representations of gene expression. Unlike prior methods, Patches excels in experimental designs with multiple attributes, such as age, treatment, and temporal dynamics, distinguishing general cellular mechanisms from condition-dependent changes. We applied Patches to both simulated data and real transcriptomic datasets from skin injury models, focusing on the effects of aging and drug treatment. Patches revealed shared wound healing patterns and condition-specific changes in cell behavior and extracellular matrix remodeling. These insights deepen our understanding of tissue repair and can identify potential biomarkers for therapeutic interventions, particularly in contexts where the experimental design is complicated by missing or difficult-to-collect data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4f4226d56b93aad71e26b307557e3ee74b991576" target='_blank'>
              Patches: A Representation Learning framework for Decoding Shared and Condition-Specific Transcriptional Programs in Wound Healing
              </a>
            </td>
          <td>
            Ozgur Beker, Dreyton Amador, Jose Francisco Pomarino Nima, Simon Van Deursen, Yvon Woappi, Bianca Dumitrascu
          </td>
          <td>2024-12-24</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The Human Cell Atlas (HCA) Consortium was founded in 2016 as an open global initiative to map each cell type in the human body and create a three-dimensional (3-D) atlas. As of December 2024, 18 Biological Networks are assembling the first draft of the HCA from organs, tissues, and organ systems, including the heart, lung, liver, and immune system. Although the completed first version of the HCA should be released within a year, possibly two, the HCA Biological Networks are making the atlases available on the HCA Data Portal as they are released. Since 2016, the Consortium has grown to include more than 3,200 members from more than 1,700 institutes and now involves 99 countries to allow data from diverse geographic and ethnic groups and age ranges. The freely available data and cell maps will help transform future healthcare by improving the understanding of tissue-specific human cell biology in health and disease. This Editorial aims to provide an update on the current status of the HCA and highlights how this encyclopedia of cells will be an important step towards providing better care to individual patients, which will benefit all of humanity.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a69759adde73d98aef73d6522852ba94f1d4ec36" target='_blank'>
              Editorial: The Human Cell Atlas. What Is It and Where Could It Take Us?
              </a>
            </td>
          <td>
            D. Parums
          </td>
          <td>2024-12-18</td>
          <td>Medical Science Monitor: International Medical Journal of Experimental and Clinical Research</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="Proteins, nature’s intricate molecular machines, are the products of billions of years of evolution and play fundamental roles in sustaining life. Yet, deciphering their molecular language - that is, understanding how protein sequences and structures encode and determine biological functions - remains a corner-stone challenge in modern biology. Here, we introduce Evolla, an 80 billion frontier protein-language generative model designed to decode the molecular language of proteins. By integrating information from protein sequences, structures, and user queries, Evolla generates precise and contextually nuanced insights into protein function. A key innovation of Evolla lies in its training on an unprecedented AI-generated dataset: 546 million protein question-answer pairs and 150 billion word tokens, designed to reflect the immense complexity and functional diversity of proteins. Post-pretraining, Evolla integrates Direct Preference Optimization (DPO) to refine the model based on preference signals and Retrieval-Augmented Generation (RAG) for external knowledge incorporation, improving response quality and relevance. To evaluate its performance, we propose a novel framework, Instructional Response Space (IRS), demonstrating that Evolla delivers expert-level insights, advancing research in proteomics and functional genomics while shedding light on the molecular logic encoded in proteins. The online demo is available at http://www.chat-protein.com/.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/41350ce555e79d9631e9a6a33015db872644596f" target='_blank'>
              Decoding the Molecular Language of Proteins with Evolla
              </a>
            </td>
          <td>
            Xibin Zhou, Chenchen Han, Yingqi Zhang, Jin Su, Kai Zhuang, Shiyu Jiang, Zichen Yuan, Wei Zheng, Fengyuan Dai, Yuyang Zhou, Yuyang Tao, Dan Wu, Fajie Yuan
          </td>
          <td>2025-01-09</td>
          <td>bioRxiv</td>
          <td>1</td>
          <td>4</td>
        </tr>

        <tr id="Precision, or personalized, medicine aims to stratify patients based on variable pathogenic signatures to optimize the effectiveness of disease prevention and treatment. This approach is favorable in the context of brain disorders, which are often heterogeneous in their pathophysiological features, patterns of disease progression and treatment response, resulting in limited therapeutic standard-of-care. Here we highlight the transformative role that human induced pluripotent stem cell (hiPSC)-derived neural models are poised to play in advancing precision medicine for brain disorders, particularly emerging innovations that improve the relevance of hiPSC models to human physiology. hiPSCs derived from accessible patient somatic cells can produce various neural cell types and tissues; current efforts to increase the complexity of these models, incorporating region-specific neural tissues and non-neural cell types of the brain microenvironment, are providing increasingly relevant insights into human-specific neurobiology. Continued advances in tissue engineering combined with innovations in genomics, high-throughput screening and imaging strengthen the physiological relevance of hiPSC models and thus their ability to uncover disease mechanisms, therapeutic vulnerabilities, and tissue and fluid-based biomarkers that will have real impact on neurological disease treatment. True physiological understanding, however, necessitates integration of hiPSC-neural models with patient biophysical data, including quantitative neuroimaging representations. We discuss recent innovations in cellular neuroscience that can provide these direct connections through generative AI modeling. Our focus is to highlight the great potential of synergy between these emerging innovations to pave the way for personalized medicine becoming a viable option for patients suffering from neuropathologies, particularly rare epileptic and neurodegenerative disorders.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/05f854b18c0e8288a4a8f1c55366790c2e18cb93" target='_blank'>
              Advances in physiological and clinical relevance of hiPSC-derived brain models for precision medicine pipelines
              </a>
            </td>
          <td>
            Negin Imani Farahani, Lisa Lin, Shama Nazir, Alireza Naderi, Leanne Rokos, A. R. McIntosh, Lisa M. Julian
          </td>
          <td>2025-01-06</td>
          <td>Frontiers in Cellular Neuroscience</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="ABSTRACT Antimicrobial peptides (AMPs) are promising candidates to combat multidrug‐resistant pathogens. However, the high cost of extensive wet‐lab screening has made AI methods for identifying and designing AMPs increasingly important, with machine learning (ML) techniques playing a crucial role. AI approaches have recently revolutionised this field by accelerating the discovery of new peptides with anti‐infective activity, particularly in preclinical mouse models. Initially, classical ML approaches dominated the field, but recently there has been a shift towards deep learning (DL) models. Despite significant contributions, existing reviews have not thoroughly explored the potential of large language models (LLMs), graph neural networks (GNNs) and structure‐guided AMP discovery and design. This review aims to fill that gap by providing a comprehensive overview of the latest advancements, challenges and opportunities in using AI methods, with a particular emphasis on LLMs, GNNs and structure‐guided design. We discuss the limitations of current approaches and highlight the most relevant topics to address in the coming years for AMP discovery and design.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9c310642e1a07abeab7c6a8216ec60328feb74a2" target='_blank'>
              AI Methods for Antimicrobial Peptides: Progress and Challenges
              </a>
            </td>
          <td>
            C. A. Brizuela, Gary Liu, Jonathan M. Stokes, César de la Fuente-Nunez
          </td>
          <td>2025-01-01</td>
          <td>Microbial Biotechnology</td>
          <td>0</td>
          <td>47</td>
        </tr>

        <tr id="Abstract Advances in three-dimensional (3D) genomics have revealed the spatial characteristics of chromatin interactions in gene expression regulation, which is crucial for understanding molecular mechanisms in biological processes. High-throughput technologies like ChIA-PET, Hi-C, and their derivatives methods have greatly enhanced our knowledge of 3D chromatin architecture. However, the chromatin interaction mechanisms remain largely unexplored. Deep learning, with its powerful feature extraction and pattern recognition capabilities, offers a promising approach for integrating multi-omics data, to build accurate predictive models of chromatin interaction matrices. This review systematically summarizes recent advances in chromatin interaction matrix prediction models. By integrating DNA sequences and epigenetic signals, we investigate the latest developments in these methods. This article details various models, focusing on how one-dimensional (1D) information transforms into the 3D structure chromatin interactions, and how the integration of different deep learning modules specifically affects model accuracy. Additionally, we discuss the critical role of DNA sequence information and epigenetic markers in shaping 3D genome interaction patterns. Finally, this review addresses the challenges in predicting chromatin interaction matrices, in order to improve the precise mapping of chromatin interaction matrices and DNA sequence, and supporting the transformation and theoretical development of 3D genomics across biological systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/52f9ba1cd4895ed845b3c4c859d3d63107830298" target='_blank'>
              A review of deep learning models for the prediction of chromatin interactions with DNA and epigenomic profiles
              </a>
            </td>
          <td>
            Yunlong Wang, Siyuan Kong, Cong Zhou, Yanfang Wang, Yubo Zhang, Yaping Fang, Guoliang Li
          </td>
          <td>2024-11-22</td>
          <td>Briefings in Bioinformatics</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="We present an approach of using AI to model and simulate biology and life. Why is it important? Because at the core of medicine, pharmacy, public health, longevity, agriculture and food security, environmental protection, and clean energy, it is biology at work. Biology in the physical world is too complex to manipulate and always expensive and risky to tamper with. In this perspective, we layout an engineering viable approach to address this challenge by constructing an AI-Driven Digital Organism (AIDO), a system of integrated multiscale foundation models, in a modular, connectable, and holistic fashion to reflect biological scales, connectedness, and complexities. An AIDO opens up a safe, affordable and high-throughput alternative platform for predicting, simulating and programming biology at all levels from molecules to cells to individuals. We envision that an AIDO is poised to trigger a new wave of better-guided wet-lab experimentation and better-informed first-principle reasoning, which can eventually help us better decode and improve life.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a51a31b87df737adcddf421c5e5592bc6c94e01a" target='_blank'>
              Toward AI-Driven Digital Organism: Multiscale Foundation Models for Predicting, Simulating and Programming Biology at All Levels
              </a>
            </td>
          <td>
            Le Song, Eran Segal, Eric P. Xing
          </td>
          <td>2024-12-09</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>2</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3498e511e9eb5c885a5bd150e27ed48f227d9e8a" target='_blank'>
              Future Directions for Quantitative Systems Pharmacology.
              </a>
            </td>
          <td>
            Birgit Schoeberl, C. Musante, S. Ramanujan
          </td>
          <td>2025-01-16</td>
          <td>Handbook of experimental pharmacology</td>
          <td>0</td>
          <td>23</td>
        </tr>

        <tr id="Multi-cellular biological systems, including the immune system, are highly complex, dynamic, and adaptable. Systems biologists aim to understand such complexity at a quantitative level. However, these ambitious efforts are often limited by access to a variety of high-density intra-, extra- and multi-cellular measurements resolved in time and space and across a variety of perturbations. The advent of automation, OMICs and single-cell technologies now allows high dimensional multi-modal data acquisition from the same biological samples multiplexed at scale (multi-OMICs). As a result, systems biologists -theoretically- have access to more data than ever. However, the mathematical frameworks and computational tools needed to analyze and interpret such data are often still nascent, limiting the biological insights that can be obtained without years of computational method development and validation. More pressingly, much of the data sits in silos in formats that are incomprehensible to other scientists or machines limiting its value to the vaster scientific community, especially the computational biologists tasked with analyzing these vast amounts of data in more nuanced ways. With the rapid development and increasing interest in using artificial intelligence (AI) for the life sciences, improving how biologic data is organized and shared is more pressing than ever for scientific progress. Here, we outline a practical approach to multi-modal data management and FAIR sharing, which are in line with the latest US and EU funders’ data sharing policies. This framework can help extend the longevity and utility of data by allowing facile use and reuse, accelerating scientific discovery in the biomedical sciences.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cbf19ba30670527f86b587b0d81dbb1659648794" target='_blank'>
              A practical guide to FAIR data management in the age of multi-OMICS and AI
              </a>
            </td>
          <td>
            Douaa Mugahid, Jared Lyon, Charlie Demurjian, Nathan Eolin, Charlie Whittaker, Mark Godek, Douglas Lauffenburger, Sarah Fortune, Stuart Levine
          </td>
          <td>2025-01-20</td>
          <td>Frontiers in Immunology</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="The complexity of human biology and its intricate systems holds immense potential for advancing human health, disease treatment, and scientific discovery. However, traditional manual methods for studying biological interactions are often constrained by the sheer volume and complexity of biological data. Artificial Intelligence (AI), with its proven ability to analyze vast datasets, offers a transformative approach to addressing these challenges. This paper explores the intersection of AI and microscopy in life sciences, emphasizing their potential applications and associated challenges. We provide a detailed review of how various biological systems can benefit from AI, highlighting the types of data and labeling requirements unique to this domain. Particular attention is given to microscopy data, exploring the specific AI techniques required to process and interpret this information. By addressing challenges such as data heterogeneity and annotation scarcity, we outline potential solutions and emerging trends in the field. Written primarily from an AI perspective, this paper aims to serve as a valuable resource for researchers working at the intersection of AI, microscopy, and biology. It summarizes current advancements, key insights, and open problems, fostering an understanding that encourages interdisciplinary collaborations. By offering a comprehensive yet concise synthesis of the field, this paper aspires to catalyze innovation, promote cross-disciplinary engagement, and accelerate the adoption of AI in life science research.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bdbb86bd3b68d8ab0a53796da78eaa6872ab2a54" target='_blank'>
              Applications and Challenges of AI and Microscopy in Life Science Research: A Review
              </a>
            </td>
          <td>
            Himanshu Buckchash, G. Verma, Dilip K. Prasad
          </td>
          <td>2025-01-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="Recent advances in single-cell RNA-Seq (scRNA-Seq) technologies have revolutionized our ability to gather molecular insights into different phenotypes, such as diseases, at the level of individual cells. The analysis of the resulting data poses significant challenges due to their sparsity and large volume, and proper statistical methods are required to analyze and extract information from scRNA-Seq datasets. Sample classification based on gene expression data has proven effective and valuable for precision medicine applications. However, standard classification schemas are often not suitable for scRNA-Seq due to its unique characteristics, and new algorithms are required to effectively analyze and classify samples at the single-cell level. In this article, we introduce singleDeep, an end-to-end pipeline that streamlines the analysis of scRNA-Seq data training deep neural networks, enabling robust prediction and characterization of sample phenotypes. To validate the effectiveness of singleDeep, we applied it to make predictions on scRNA-Seq datasets from different conditions, including systemic lupus erythematosus and Alzheimer’s disease. Our results demonstrate strong diagnostic performance, validated both internally and externally. Moreover, compared with traditional machine learning methods applied to pseudobulk data, singleDeep consistently outperformed these approaches. In addition to prediction accuracy, singleDeep provides valuable insights into cell types and gene importance estimation for phenotypic characterization. This functionality provided additional and valuable information in our use cases. For instance, we corroborated that some interferon signature genes are consistently relevant for autoimmunity across all immune cell types in lupus. On the other hand, we discovered that genes linked to dementia have relevant roles in specific brain cell populations, such as APOE in astrocytes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ff142c43b5d221ba3b5d2f183dc21822545fcc71" target='_blank'>
              Explainable deep neural networks for predicting sample phenotypes from single-cell transcriptomics
              </a>
            </td>
          <td>
            Jordi Martorell-Marugán, R. López-Domínguez, J. Villatoro-García, D. Toro-Domínguez, M. Chierici, G. Jurman, P. Carmona-Sáez
          </td>
          <td>2024-11-22</td>
          <td>Briefings in Bioinformatics</td>
          <td>0</td>
          <td>28</td>
        </tr>

        <tr id="The complex interactions between biological entities provide valuable insights into fundamental life processes, which pave the way to a deeper understanding of disease mechanisms for the development of innovative therapeutic strategies. To enable large-scale predictions of biological interactions, multifarious AI-driven predictors have been developed. However, most of these are developed by leveraging information from only a limited subset of interaction types, and the broader interaction types landscape could facilitate AI algorithms to learn more informative patterns to predict unknown interactions. To address the need of a robust and precise interaction predictor, we introduce BIND (Biological Interaction Network Discovery), a predictor that leverages simultaneous learning across 10 biological entities and 30 interaction types. To develop BIND, we first evaluate 11 distinct Knowledge Graph Embedding Methods on the largest public biomedical interaction dataset namely PrimeKG. For each relation type, we extracted entity embeddings from the top 5 performing Knowledge Graph Embedding Models (KGEMs) and input them into 7 distinct machine learning classifiers. Rigorous evaluation of 1,050 predictive pipelines demonstrated that specific combinations of KGEMs and classifiers achieved F1 scores of 90% to 99% across various interaction types. Comprehensive evaluation across each relation type identified the top-performing predictive pipelines, which became the foundation of the BIND web application. To reveal the practical utility of our web application in identifying novel biological interactions, we conducted a case study on drug-phenotype interactions. The application gave 1,355 high confidence predictions, from which potential interactions were subsequently validated by scientific evidence found within the existing literature (Table 5). We believe BIND’s web application’s public access will serve as a valuable tool for biologists to identify unknown interactions that can be subsequently validated through wet-lab experiments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/59f21a1ff67df797c6bbaabd975ec6cdb6421507" target='_blank'>
              BIND: Large-Scale Biological Interaction Network Discovery through Knowledge Graph-Driven Machine Learning
              </a>
            </td>
          <td>
            Naafey Aamer, M. Asim, Aamer Iqbal Bhatti, Andreas Dengel
          </td>
          <td>2025-01-19</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/89f4dafce3e68eb14cab151613eca45fd2ce0bb6" target='_blank'>
              Opportunities and challenges of single-cell and spatially resolved genomics methods for neuroscience discovery.
              </a>
            </td>
          <td>
            B. Bonev, Castelo-Branco Gonçalo, Fei Chen, Simone Codeluppi, M. Corces, Jean Fan, Myriam Heiman, Kenneth D. Harris, Fumitaka Inoue, M. Kellis, Ariel Levine, M. Lotfollahi, Chongyuan Luo, Kristen R. Maynard, Mor Nitzan, Vijay Ramani, Rahul Satijia, Lucas Schirmer, Yin Shen, Na Sun, G. Green, F. Theis, Xiao Wang, Joshua D. Welch, Ozgun Gokce, Genevieve Konopka, SA Liddelow, Evan Z. Macosko, O. Bayraktar, Naomi Habib, T. Nowakowski
          </td>
          <td>2024-12-01</td>
          <td>Nature neuroscience</td>
          <td>3</td>
          <td>37</td>
        </tr>

        <tr id="RNA velocity has emerged as a popular approach for modeling cellular change along the phenotypic landscape but routinely omits regulatory interactions between genes. Conversely, methods that infer gene regulatory networks (GRNs) do not consider the dynamically changing nature of biological systems. To integrate these two currently disconnected fields, we present RegVelo, an end-to-end dynamic, interpretable, and actionable deep learning model that learns a joint model of splicing kinetics and gene regulatory relationships and allows us to perform in silico perturbation predictions. When applied to datasets of the cell cycle, human hematopoiesis, and murine pancreatic endocrinogenesis, RegVelo demonstrates superior predictive power for interactions and perturbation simulations, for example, compared to methods that focus solely on dynamics or GRN inference. To leverage RegVelo’s full potential, we studied the dynamics of zebrafish neural crest development and underlying regulatory mechanisms using our Smart-seq3 dataset and shared gene expression and chromatin accessibility measurements. Using RegVelo’s in silico perturbation predictions, validated by CRISPR/Cas9-mediated knockout and single-cell Perturb-seq, we establish transcription factor tfec as an early driver and elf1 as a novel regulator of pigment cell fate and propose a gene-regulatory circuit involving tfec and elf1 interactions via the toggle-switch model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cb495aa52d71e37d301ee44c41f833b2bc276eae" target='_blank'>
              RegVelo: gene-regulatory-informed dynamics of single cells
              </a>
            </td>
          <td>
            Weixu Wang, Zhiyuan Hu, P. Weiler, Sarah Mayes, Marius Lange, Jingye Wang, Zhengyuan Xue, Tatjana Sauka-Spengler, F. Theis
          </td>
          <td>2024-12-11</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Advancements in single-cell RNA sequencing (scRNA-seq) have enabled the analysis of millions of cells, but integrating such data across samples and methods while mitigating batch effects remains challenging. Deep learning approaches address this by learning biologically conserved gene expression representations, yet systematic benchmarking of loss functions and integration performance is lacking. This study evaluated 16 integration methods using a unified variational autoencoder framework, incorporating batch and cell-type information. Results revealed limitations in the single-cell integration benchmarking index (scIB) for preserving intra-cell-type information. To address this, we introduced a correlation-based loss function and enhanced benchmarking metrics to better capture biological conservation. Using annotations from the Human Lung Cell Atlas and Human Fetal Lung Cell Atlas, our approach improved biological signal preservation. This work highlights the need for biologically informed metrics in scRNA-seq integration and offers guidance for future deep learning developments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cc37ff87c365ae12dc4c516220b1d15af5c81cd1" target='_blank'>
              Benchmarking deep learning methods for biologically conserved single-cell integration
              </a>
            </td>
          <td>
            Chenxin Yi, Jinyu Cheng, Wanquan Liu, Junwei Liu, Yixue Li
          </td>
          <td>2024-12-13</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Modeling genetic perturbations and their effect on the transcriptome is a key area of pharmaceutical research. Due to the complexity of the transcriptome, there has been much excitement and development in deep learning (DL) because of its ability to model complex relationships. In particular, the transformer-based foundation model paradigm emerged as the gold-standard of predicting post-perturbation responses. However, understanding these increasingly complex models and evaluating their practical utility is lacking, along with simple but appropriate benchmarks to compare predictive methods. Here, we present a simple baseline method that outperforms both state of the art (SOTA) in DL and other proposed simpler neural architectures, setting a necessary benchmark to evaluate in the field of post-perturbation prediction. We also elucidate the utility of foundation models for the task of post-perturbation prediction via generalizable fine-tuning experiments that can be translated to different applications of transformer-based foundation models to tasks of interest. Furthermore, we provide a corrected version of a popular dataset used for benchmarking perturbation prediction models. Our hope is that this work will properly contextualize further development of DL models in the perturbation space with necessary control procedures.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/aba98d5e0a98953815d95f1052d33e42a010944f" target='_blank'>
              Simple controls exceed best deep learning algorithms and reveal foundation model effectiveness for predicting genetic perturbations
              </a>
            </td>
          <td>
            Daniel R. Wong, Abby S. Hill, Rob Moccia
          </td>
          <td>2025-01-08</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Perturb-seq is a technique that combines scRNA-seq and CRISPR to explore cellular system operations and disease-associated genes, providing profound insights into the mechanisms behind biological processes. Although powerful, such a method is limited by its scalability for its cost-intensive and time-consuming nature, which calls for in silico prediction of genetic perturbation responses. Among all computational methods, GEARS represents the state-of-the-art by explicitly modeling the response of each gene to the perturbed gene, exploiting gene-gene relationships derived from Gene Ontology annotations. However, our evaluation of Gene Ontology annotations indicated that they are insufficient as the sole source of prior knowledge for predicting genetic perturbation responses. Therefore, they cannot fully support predicting genetic perturbation responses. We addressed this gap by constructing an augmented gene ontology network that incorporates extensive knowledge of diseases, drugs, and genes to capture nuanced gene-gene relationships not indicated by Gene Ontology alone. By replacing only the Gene Ontology graph in GEARS, our method outperforms GEARS in both single gene and combinational perturbation predictions. These findings suggest the effectiveness and importance of incorporating finer prior knowledge in predicting genetic perturbation responses, thereby encouraging future works on improving knowledge representation for single-cell perturbation prediction.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/37ff9072ceb45f79b93ddf328416db61c6d41b9a" target='_blank'>
              Improving Genetic Perturbation Response Prediction with an Enhanced Biological Knowledge Graph
              </a>
            </td>
          <td>
            Yang Li, Rui-Jia Chen, Yuting Tan, Xue Zhong, Bingshan Li, Zhijun Yin
          </td>
          <td>2024-12-03</td>
          <td>2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In this perspective, we will discuss the impact of some of the most recent advancements in materials discovery, particularly focusing on the role of robotics, artificial intelligence, and self-driving laboratories, as well as their implications for the Swiss research landscape. While it seems timely to aim for broad, revolutionary breakthroughs in this field, we argue that more incremental steps - such as, for example, fully automatic grinding of solid powders or fully automated Rietveld refinements - may have a more significant impact on materials discovery, at least in the short run. In the center of these considerations is how small, interdisciplinary groups can drive significant progress by contributing targeted innovations, such as e.g.robotic sample preparation or computational predictions. Additionally, given the large investments that are necessary for future infrastructures in materials discovery, we discuss the potential case for the establishment - in the long run - of a national infrastructure, a Swiss Materials Discovery Lab, to support automated material synthesis and advanced characterization, ultimately accelerating innovation in both academic and industrial settings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a3b324a43b180579d09c4e65131c1393972e770a" target='_blank'>
              The Changing Landscape of Materials Discovery.
              </a>
            </td>
          <td>
            F. O. Von Rohr
          </td>
          <td>2024-12-18</td>
          <td>Chimia</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Creating end-to-end bioinformatics workflows requires diverse domain expertise, which poses challenges for both junior and senior researchers as it demands a deep understanding of both genomics concepts and computational techniques. While large language models (LLMs) provide some assistance, they often fall short in providing the nuanced guidance needed to execute complex bioinformatics tasks, and require expensive computing resources to achieve high performance. We thus propose a multi-agent system built on small language models, fine-tuned on bioinformatics data, and enhanced with retrieval augmented generation (RAG). Our system, BioAgents, enables local operation and personalization using proprietary data. We observe performance comparable to human experts on conceptual genomics tasks, and suggest next steps to enhance code generation capabilities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/165dca9b1794ae105ba4fc5c984971deedd33bd4" target='_blank'>
              BioAgents: Democratizing Bioinformatics Analysis with Multi-Agent Systems
              </a>
            </td>
          <td>
            Nikita Mehandru, Amanda K. Hall, Olesya Melnichenko, Yulia Dubinina, Daniel Tsirulnikov, David Bamman, Ahmed Alaa, Scott Saponas, V. Malladi
          </td>
          <td>2025-01-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>34</td>
        </tr>

        <tr id="
 With the adoption of Foundation Models (FMs), Artificial Intelligence (AI) has become increasingly significant in bioinformatics and has successfully addressed many historical challenges, such as pre-training frameworks, model evaluation, and interpretability. FMs demonstrate notable proficiency in managing large-scale, unlabeled datasets, because experimental procedures are costly and labor-intensive. In various downstream tasks, FMs have consistently achieved noteworthy results, demonstrating high levels of accuracy in representing biological entities. A new era in computational biology has been ushered in by the application of FMs, focusing on both general and specific biological issues. In this review, we introduce recent advancements in bioinformatics FMs that employed in a variety of downstream tasks, including genomics, transcriptomics, proteomics, drug discovery, and single cell analysis. Our aim is to assist scientists in selecting appropriate FMs in bioinformatics, according to four model types: language FMs, vision FMs, graph FMs, and multimodal FMs. In addition to understanding molecular landscapes, AI technology can establish the theoretical and practical foundation for continued innovation in molecular biology.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3f21f2fadc60b7f627b5154a9944ec22d28c2678" target='_blank'>
              Foundation models in bioinformatics
              </a>
            </td>
          <td>
            Fei Guo, Renchu Guan, Yaohang Li, Qi Liu, Xiaowo Wang, Can Yang, Jianxin Wang
          </td>
          <td>2025-01-25</td>
          <td>National Science Review</td>
          <td>0</td>
          <td>25</td>
        </tr>

        <tr id="Deep Science is enabling high-throughput experimentation (HTE) to design novel biological entities with desired properties. E.g., blood-brain-barrier (BBB) crossing adeno-associated virus (AAV) vectors, needed for systemic delivery of gene therapies to brain cells, have been identified through innovative directed evolution assays such as M-CRE-ATE and TRACER. But, even these high-throughput experiments are only able to explore a miniscule portion of the large design space of biological entities. In this paper, we introduce autograd based maximization of protein fitness (AutoMaxProFit) to learn from and improve upon protein designs generated with high-throughput screens. Using a transformer based generative AI network and protein language models, we improve upon the design of a variant previously discovered through HTE, to yield 4x better enrichment in brain endothelial cells, as estimated by molecular dynamics (MD) simulations. This shows that Deep Tech models can learn from the observations generated by Deep Science experiments and go on to find more optimal design candidates for application in Biopharma.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9dcf39bd0d7d45daee56cecbe3684d2260d2a64e" target='_blank'>
              Learning from and improving upon high-throughput screens for protein fitness with Generative AI - Application to BBB-crossing AAV design
              </a>
            </td>
          <td>
            Ayan Kashyap, Kumari Soniya, Anubhooti, Subhalakshmi Kandavel, Siva Kanishka, Manasvi Sharma, Prasad Chodavarapu
          </td>
          <td>2024-12-11</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

    </tbody>
    <tfoot>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
      </tr>
    </tfoot>
    </table>
    </p>
  </div>

</body>

<script>

  function create_author_list(author_list) {
    let td_author_element = document.getElementById();
    for (let i = 0; i < author_list.length; i++) {
          // tdElements[i].innerHTML = greet(tdElements[i].innerHTML);
          alert (author_list[i]);
      }
  }

  var trace1 = {
    x: ['2024', '2025'],
    y: [5, 1],
    name: 'Num of citations',
    yaxis: 'y1',
    type: 'scatter'
  };

  var data = [trace1];

  var layout = {
    yaxis: {
      title: 'Num of citations',
      }
  };
  Plotly.newPlot('myDiv1', data, layout);
</script>
<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;

                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);

                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';

                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: true,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '7%', targets: 0 },
        { width: '30%', targets: 1 },
        { width: '25%', targets: 2 },
        { width: '15%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  new DataTable('#table2', dataTableOptions);

  var table1 = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table1.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
    }
  });
  var table2 = $('#table2').DataTable();
  $('#table2 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table2.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
    }
  });
</script>
<style>
  .child-row-content {
    text-align: justify;
    text-justify: inter-word;
    word-wrap: break-word; /* Ensure long words are broken */
    white-space: normal; /* Ensure text wraps to the next line */
    max-width: 100%; /* Ensure content does not exceed the table width */
    padding: 10px; /* Optional: add some padding for better readability */
    /* font size */
    font-size: small;
  }
</style>
</html>







  
  




  



                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.top", "navigation.tabs"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    

      <script src="../assets/javascripts/bundle.c8d2eff1.min.js"></script>
      
    
<script>
  // Execute intro.js when a button with id 'intro' is clicked
  function startIntro(){
      introJs().setOptions({
          tooltipClass: 'customTooltip'
      }).start();
  }
</script>
<script>
  

  // new DataTable('#table1', {
  //   order: [[5, 'desc']],
  //   "columnDefs": [
  //       {"className": "dt-center", "targets": "_all"},
  //       { width: '30%', targets: 0 }
  //     ],
  //   pageLength: 10,
  //   layout: {
  //       topStart: {
  //           buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
  //       }
  //   }
  // });

  // new DataTable('#table2', {
  //   order: [[3, 'desc']],
  //   "columnDefs": [
  //       {"className": "dt-center", "targets": "_all"},
  //       { width: '30%', targets: 0 }
  //     ],
  //   pageLength: 10,
  //   layout: {
  //       topStart: {
  //           buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
  //       }
  //   }
  // });
  new DataTable('#table3', {
    initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    // order: [[3, 'desc']],
    "columnDefs": [
        {"className": "dt-center", "targets": "_all"},
        { width: '30%', targets: 0 }
      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  });
  new DataTable('#table4', {
    initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    // order: [[3, 'desc']],
    "columnDefs": [
        {"className": "dt-center", "targets": "_all"},
        { width: '30%', targets: 0 }
      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  });
</script>


  </body>
</html>